<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback">
  <meta name="keywords" content="Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link href="/assets/images/icon.png" rel="icon" sizes="32x32" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="./static/js/target.js"></script> -->
  <!-- <base target="_blank"> -->
</head>
<body>


<!-- Navigation bar for visiting other research pages -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://1jsingh.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://1jsingh.github.io/gradop">
            GradOP+ (CVPR-2023)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/paint2pix">
            Paint2pix (ECCV-2022)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/intelli-paint">
            Intelli-paint (ECCV-2022)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/semantic-guidance">
            Semantic-Guidance (CVPR-2021)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback <br> NeurIPS 2023</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://1jsingh.github.io/">Jaskirat Singh</a><sup>1</sup>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vNHqr3oAAAAJ&hl=en">Liang Zheng</a><sup>1,2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Australian National University&emsp;&emsp;</span>
            <span class="author-block"><sup>2</sup>Australian Centre for Robotic Vision</span>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Authors<br></span> 
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- png Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.04749.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.04749.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Yk83RPCOa2o&ab_channel=anucvml"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://github.com/1jsingh/Divide-Evaluate-and-Refine"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-paint-brush"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/1jsingh/Divide-Evaluate-and-Refine"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#citation"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Cite</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" text-align: center>
      <img src="./docs/divide-evaluate-and-refine/overview-iter-v1.png" width=100%/>  
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class=" has-text-justified">
        <!-- <br>  -->
        <b>Overview.</b> We propose a training-free decompositional framework which helps both better evaluate and gradually improve text-to-image alignment using iterative VQA feedback.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- sample results -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h3 class="subtitle has-text-centered">
        <b>Sample Results (Stable Diffusion)</b>
      </h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p4.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p3.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p5.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p6.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p7.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p2.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/divide-evaluate-and-refine/results-v1-p1.png">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop alternate-section">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <p>The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. 
              While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. 
              Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as <em>CLIP</em>.</p>

            <p>To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. 
              In particular, we first introduce a <em><strong>D</strong>ecompositional-<strong>A</strong>lignment-<strong>S</strong>core</em> which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. 
              Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. 
              Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional <em>CLIP, BLIP</em> scores. </p>

              
              <p>Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expressivity of different assertions in the final image outputs.
                Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy.</p>

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
</section>


<!-- other sections -->
<section class="hero">
<div class="hero-body is-small">
<div class="container alternate-section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluating Text-to-Image Alignment</h2>
        <figure>
          <img src="./docs/divide-evaluate-and-refine/overview-eval-v3.png" width=100%/>
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
        </figure>
        <p style="text-align:justify"> 
          <b>Decompositional Alignment Score:</b> Traditional methods for evaluating text-to-image alignment, for example, CLIP<sup><a href="#ref1">1</a></sup>, BLIP-2<sup><a href="#ref2">2</a></sup> and BLIP2-ITM 
            (which provides a binary image-text matching score between 0 and 1) often fail to distinguish between good (<em>right</em>) and bad (<em>left</em>) image outputs and can give high scores even 
            if the generated image is not an accurate match for input prompt (missing yellow car). In contrast, by breaking down the prompt into a set of disjoint assertions and then evaluating their alignment with the 
            generated image using a VQA model<sup><a href="#ref2">2</a></sup>, the proposed Decompositional-Alignment Score (DA-score) shows much better correlation with human ratings.
        </p>
        <br><br> <br><br>
        <figure>
          <img src="./docs/divide-evaluate-and-refine/corr-human-v1.png" width=100%/>
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
        </figure>
        <p style="text-align:justify"> 
          <br>
          <b>Method comparisons w.r.t correlation with human ratings:</b>
          We compare the correlation of different text-to-image alignment scores with those obtained from human subjects, as the number of subjects in the input prompt is varied. 
          We observe that the proposed alignment score (DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores like CLIP, BLIP and BLIP2.
          <br><br>
        </p>
      </div>
  </div>
</div>
</div>

<section class="hero">
  <div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Improving Text-to-Image Alignment</h2>
          <br><br>
          <figure>
            <img src="./docs/divide-evaluate-and-refine/method-overview-v1.png" width=100%/>
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
          </figure>
          <p style="text-align:justify"> 
            <br>
            <b>Method Overview:</b>
            Given a text prompt <b>P</b> and an initially generated output <b>I</b>, we first generate a set of disjoint assertions <span>a<sub>i</sub></span> regarding the content of the caption. The alignment of the output image <b>I</b> with each of these assertions is then calculated using a VQA model. Finally, we use the assertion-based-alignment scores <span>u<sub>i</sub></span>(<b>I</b>,<b>P</b>) as feedback to increase the weightage <span>w<sub>i</sub></span> (of the assertion with least alignment score) in a parameterized diffusion model formulation. This process can then be performed in an iterative manner to gradually improve the quality of the generated outputs until a desirable threshold for the overall alignment score Omega(<b>I</b><sub>k</sub>,<b>P</b>) is reached.                    
            <br><br>
          </p>
        </div>
    </div>
  </div>
  </div>

  

<section class="hero">
  <div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <figure>
            <img src="./docs/divide-evaluate-and-refine/alignment-acc-var-diff-v1.png" width=100%/>
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
          </figure>
          <p style="text-align:justify"> 
            <br>
            <b>Variation of alignment accuracy with prompt difficulty.</b> We observe that while the accuracy of all methods decreases with increasing prompt difficulty, the proposed iterative refinement approach consistently performs better than prior works.            
            <br><br>
          </p>
        </div>
    </div>
  </div>
  </div>



<!--/ Paper video. -->
<!-- </div> -->
</section>




<!-- Bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title" id="citation">BibTeX</h2>
    <span>If you find our work useful in your research, please consider citing: </span> 
    <pre><code>@inproceedings{singh2023divide,
      title={Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback},
      author={Singh, Jaskirat and Zheng, Liang},
      booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
      year={2023}
    }
  </code></pre>
  </div>
</section>

</section>
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.png">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project. If you want to reuse their source code, please credit them.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
