<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback">
  <meta name="keywords" content="Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link href="/assets/images/icon.png" rel="icon" sizes="32x32" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="./static/js/target.js"></script> -->
  <!-- <base target="_blank"> -->
</head>
<body>


<!-- Navigation bar for visiting other research pages -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://1jsingh.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://1jsingh.github.io/gradop">
            GradOP+ (CVPR-2023)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/paint2pix">
            Paint2pix (ECCV-2022)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/intelli-paint">
            Intelli-paint (ECCV-2022)
          </a>
          <a class="navbar-item" href="https://1jsingh.github.io/semantic-guidance">
            Semantic-Guidance (CVPR-2021)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Divide, Evaluate, and Refine: Evaluating and
            Improving Text-to-Image Alignment with
            Iterative VQA Feedback</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://1jsingh.github.io/">Jaskirat Singh</a><sup>1</sup>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vNHqr3oAAAAJ&hl=en">Liang Zheng</a><sup>1,2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Australian National University&emsp;&emsp;</span>
            <span class="author-block"><sup>2</sup>Australian Centre for Robotic Vision</span>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Authors<br></span> 
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- png Link. -->
              <span class="link-block">
                <a href="https://1jsingh.github.io/publications/gradopt-guided-synthesis.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://1jsingh.github.io/publications/gradopt-guided-synthesis.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Yk83RPCOa2o&ab_channel=anucvml"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-paint-brush"></i>
                  </span>
                  <span>Demo (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Cite</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" text-align: center>
      <img src="./docs/divide-evaluate-and-refine/overview-iter-v1.png" width=100%/>  
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class=" has-text-justified">
        <!-- <br>  -->
        <b>Overview.</b> We propose a novel iterative VQA feedback mechanism to improve text-to-image alignment.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- sample results -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h3 class="subtitle has-text-centered">
        <b>Interactive Results (Stable Diffusion)</b>
      </h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p5.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p4.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p6.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p7.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p1.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p2.png">
        </div>
        <div class="item-simple item-steve">
          <img src="./docs/gradop/sample-results-sdedit-p3.png">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop alternate-section">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <p>The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. 
              While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. 
              Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as <em>CLIP</em>.</p>

            <p>To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. 
              In particular, we first introduce a <em><strong>D</strong>ecompositional-<strong>A</strong>lignment-<strong>S</strong>core</em> which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. 
              Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. 
              Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional <em>CLIP, BLIP</em> scores. </p>

              
              <p>Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expressivity of different assertions in the final image outputs.
                Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy.</p>

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
</section>


<!-- other sections -->
<section class="hero">
<div class="hero-body is-small">
<div class="container alternate-section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluating Text-to-Image Alignment</h2>
        <figure>
          <img src="./docs/divide-evaluate-and-refine/corr-human-v1.png" width=100%/>
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
        </figure>
        <p style="text-align:justify"> 
          <br>
          <b>Method comparisons w.r.t correlation with human ratings:</b>
          We compare the correlation of different text-to-image alignment scores with those obtained from human subjects, as the number of subjects in the input prompt is varied. 
          We observe that the proposed alignment score (DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores like CLIP, BLIP and BLIP2.
          <br><br>
        </p>
      </div>
  </div>
</div>
</div>

<section class="hero">
  <div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method Overview</h2>
          <figure>
            <img src="./docs/divide-evaluate-and-refine/overview-eval-v3.png" width=100%/>
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
          </figure>
          <br><br>
          <figure>
            <img src="./docs/divide-evaluate-and-refine/method-overview-v1.png" width=100%/>
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
          </figure>
          <p style="text-align:justify"> 
            <br>
            <b>Method comparisons w.r.t correlation with human ratings:</b>
            We compare the correlation of different text-to-image alignment scores with those obtained from human subjects, as the number of subjects in the input prompt is varied. 
            We observe that the proposed alignment score (DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores like CLIP, BLIP and BLIP2.
            <br><br>
          </p>
        </div>
    </div>
  </div>
  </div>

  

<section class="hero">
  <div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Improving Text-to-Image Alignment</h2>
          <figure>
            <img src="./docs/divide-evaluate-and-refine/alignment-acc-var-diff-v1.png" width=100%/>
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->    
          </figure>
          <p style="text-align:justify"> 
            <br>
            <b>Method comparisons w.r.t correlation with human ratings:</b>
            We compare the correlation of different text-to-image alignment scores with those obtained from human subjects, as the number of subjects in the input prompt is varied. 
            We observe that the proposed alignment score (DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores like CLIP, BLIP and BLIP2.
            <br><br>
          </p>
        </div>
    </div>
  </div>
  </div>

<div class="hero-body is-small">
<div class="container alternate-section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Guided Image Synthesis from User-scribbles</h2>
        <p style="text-align:justify"><b>Comparison with prior works.</b> As compared to prior works, our method provides a more practical approach for improving output realism (with respect to the target domain) while still maintaining the faithfulness with the reference painting.
          <br><br> </p>
        <figure>
          <!-- <img src="./docs/gradop/test.png" width=100%/> -->
          <img src="./docs/gradop/sdedit-var-p4-v3.png" width=100%/>
          <img src="./docs/gradop/sdedit-var-p7.png" width=100%/>
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
        </figure>

        <p style="text-align:justify">
          <br><br>
          <b>More Results.</b> Our approach allows the user to easily generate realistic image outputs across a range of data modalities.
          <br><br> </p>
        <figure>
          <img src="./docs/gradop/sample-results-ours-p3.png" width=100%/>
          <img src="./docs/gradop/sample-results-ours-p2.png" width=100%/>
          <img src="./docs/gradop/sample-results-ours-v3.png" width=100%/>
          <!-- <img src="./docs/gradop/sdedit-var-p4-v3.png" width=100%/> -->
          <!-- <img src="./docs/gradop/sdedit-var-p7.png" width=100%/> -->
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
        </figure>
        <!-- <p style="text-align:left"><br><br>Here is a sample reference on using the provided demo for achieving a diverse range of real image edits.<br><br></p>
        <video id="teaser" controls autoplay muted loop playsinline height="100%">
          <source src="docs/demo_videos/real-image-editing-v1.mp4"
                  type="video/mp4">
        </video> -->
      </div>
  </div>
</div>
</div>

<div class="hero-body is-small">
<div class="container  alternate-section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Controlling Semantics of Different Painting Regions</h2>
        <p style="text-align:justify">
          <!-- Finally, while the above approximate guided image synthesis algorithm allows for generation of image outputs 
          with high <i>faithfulness</i> and <i>realism</i>, the semantics of different painted regions are inferred in an implicit manner.    
          Such inference is typically based on the cross-attention priors (learned by the diffusion model) 
          and might not accurately reflect the user's intent in drawing a particular painting region.
          For instance, in the first example from following figure, we note that for different outputs, 
          the blue region can be inferred as a river, waterfall, or a valley. Also note that some painting regions might be entirely omitted (<i>e.g.,</i> the brown strokes for the hut), 
          if the model does not understand that the corresponding strokes indicate a distinct semantic entity <i>e.g.,</i> a hut, small castle <i>etc</i>. Moreover, as shown below
          such discrepancies persist even if the corresponding semantic labels (<i>e.g.</i> a hut) are added to the textual prompt.  -->
          <!-- While the above approach for guided image synthesis with user-strokes allows for generation of  image outputs 
          with high <i>faithfulness</i> and <i>realism</i>,  -->
          While performing guided image synthesis with coarse user-scribbles,
          the semantics of different painted regions are inferred in an implicit manner. For instance, in following figure, we note that for different outputs, 
          the blue region can be inferred as a river, waterfall, or a valley. Also note that some painting regions might be entirely omitted (<i>e.g.,</i> the brown strokes for the hut), 
          if the model does not understand that the corresponding strokes indicate a distinct semantic entity.
           <!-- <i>e.g.,</i> a hut, small castle <i>etc</i>.  -->
          <!-- Moreover, as shown below such discrepancies persist even if the corresponding semantic labels (<i>e.g.</i> a hut) are added to the textual prompt.  -->
          <br> <br></p>
        <figure>
          <img src="./docs/gradop/semantic-control-p1.png" width=100%/>
          <!-- <img src="./docs/gradop/semantic-control-p2.png" width=100%/> -->
          <!-- <img src="./docs/gradop/semantics-control-v3.png" width=100%/> -->
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
        </figure>
        <p style="text-align:justify">
          <br> <br>
          We show that by simply defining a cross-attention based corrrespondence between input text-tokens and reference painting, the user can control the semantics of different painting regions without needing any additonal training or finetuning.
          <br> <br></p>
        <figure>
          <img src="./docs/gradop/semantic-control-p2.png" width=100%/>
          <!-- <img src="./docs/gradop/semantic-control-p2.png" width=100%/> -->
          <!-- <img src="./docs/gradop/semantics-control-v3.png" width=100%/> -->
          <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
        </figure>
      </div>
  </div>
</div>
</div>


<div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Performance on Out-of-Distribution Prompts</h2> -->
          <h2 class="title is-3">Out-of-Distribution Generalization</h2>
          <p style="text-align:justify">
            As shown in above, we find that the proposed approach allows for a high level of semantic control (both color composition and fine-grain semantics) over the output image attributes, while still maintaining the <i>realism</i> with respect to the target domain. Thus a natural question arises: <i>Can we use the proposed approach to generate realistic photos with out-of-distribution text prompts?</i>
            As shown below, we observe that both success and failure cases exist for out-of-distribution prompts. For instance, while the model was able to generate <i>"realistic photos of cats with six legs"</i>
            (note that for the same inputs prior works either 
            generate faithful but cartoon-like outputs, or, simply generate regular cats), 
            it shows poor performance while generating <i>"a photo of a rat chasing a lion"</i>.
            <br><br> </p>
          <figure>
            <img src="./docs/gradop/out-of-dist-p1.png" width=70%/>
            <img src="./docs/gradop/out-of-dist-p2.png" width=70%/>
            <!-- <img src="./docs/gradop/sdedit-var-p7.png" width=100%/> -->
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
          </figure>
          <!-- <p style="text-align:left"><br><br>Here is a sample reference on using the provided demo for achieving a diverse range of real image edits.<br><br></p>
          <video id="teaser" controls autoplay muted loop playsinline height="100%">
            <source src="docs/demo_videos/real-image-editing-v1.mp4"
                    type="video/mp4">
          </video> -->
        </div>
    </div>
  </div>
</div>

<div class="hero-body is-small">
  <div class="container alternate-section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Performance on Out-of-Distribution Prompts</h2> -->
          <h2 class="title is-3">Variation with Number of Gradient Steps</h2>
          <!-- <h2 class="title is-3">Increasing Faithfulness With Reference Painting</h2> -->
          <p style="text-align:justify">
            <!-- While our guided synthesis framework allows the user to generate realistic content from fairly coarse user-scribbles, the user can 
            also increase the faithfulness with the input strokes (<i>e.g.</i> as desired with semantic segmentation based conditioning) by simply increasing the number of
            gradient descent steps used for solving the constrained optimization problem. -->
            A key component of the proposed method is to obtain an approximate solution for the constrained problem formulation (discussed above)
            using simple gradient descent. In the following figure, we visualize the variation in generated outputs as the number of gradient descent steps used for
            performing the optimization are increased.
            <br>

            <br><br> </p>
          <figure>
            <img src="./docs/gradop/ngrad-var-final-v1.png" width=70%/>
            <!-- <img src="./docs/gradop/out-of-dist-p2.png" width=80%/> -->
            <!-- <img src="./docs/gradop/sdedit-var-p7.png" width=100%/> -->
            <!-- <figcaption>Fig.1 - Trulli, Puglia, Italy.</figcaption> -->
          </figure>

          <p style="text-align:justify">
            <br>
            <!-- <br> -->
            As shown above, we find that for <i>N=0</i>, the generated outputs are sampled randomly from the subspace of outputs conditioned only on the text.
            As the number of gradient-descent steps increase, the model converges to a subset of solutions 
            within the target subspace (conditioned only on text-prompt) which exhibit higher <i>faithfulness</i> with 
            the provided reference painting. Please note that this behaviour is in contrast with prior works like SDEdit, 
            wherein the increase in <i>faithfulness</i> to the reference is corresponded with a decrease in the <i>realism</i> of 
            the generated outputs.
            <br>
            <br><br> </p>
          
          <!-- <p style="text-align:left"><br><br>Here is a sample reference on using the provided demo for achieving a diverse range of real image edits.<br><br></p>
          <video id="teaser" controls autoplay muted loop playsinline height="100%">
            <source src="docs/demo_videos/real-image-editing-v1.mp4"
                    type="video/mp4">
          </video> -->
        </div>
    </div>
  </div>
</div>

<!--/ Paper video. -->
<!-- </div> -->
</section>




<!-- Bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <span>If you find our work useful in your research, please consider citing:</span> 
    <pre><code>@inproceedings{singh2023high,
      title={High-Fidelity Guided Image Synthesis With Latent Diffusion Models},
      author={Singh, Jaskirat and Gould, Stephen and Zheng, Liang},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      pages={5997--6006},
      year={2023}
    }
  </code></pre>
  </div>
</section>

</section>
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.png">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project. If you want to reuse their source code, please credit them.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
