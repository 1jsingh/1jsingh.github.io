<!----------------------------------- publications ----------------------------------->

<div class="featured_work" id="publications">
	<h1>Publications</h1> 
	<!-- <a href="#projects"><i class="fas fa-link"></i></a> -->
	<table class="featured-work_table">
		<tbody>
			<tr class="work_row">
				<td class="work_col_1">
					<a href="https://arxiv.org/abs/2011.12589" target="_blank"><img alt="semantic_guidance" class="project_logo" src="assets/publications/images/model_design.png"></a>
				</td>
				<td class="work_col_2">
					<h3> Combining Semantic Guidance and Deep Reinforcement Learning for Generating Human Level Paintings </h3>
					<!-- <ul>
						<li>Trained a Deep Reinforcement Learning Agent to navigate a banana world simulated in the Unity Environment.</li>
						<li>The underlying model was a Dueling Double Deep Q Network (DDQN) with prioritized experience replay.</li>
					</ul> -->
					<b>Jaskirat Singh</b> and <a href="https://scholar.google.com/citations?user=vNHqr3oAAAAJ&hl=en" target="_blank">Liang Zheng</a>, 2020 <br><br>
					[<a href="/assets/publications/semantic_guidance.pdf" target="_blank">Paper</a>] <br><br>

					<div align="justify"><small class="abstract"><b>Abstract:</b> Generation of stroke-based non-photorealistic imagery, is an important problem in the computer vision community. As an endeavor in this direction, substantial recent research efforts have been focused on teaching machines ``how to paint'', in a manner similar to a human painter. However, the applicability of previous methods has been limited to datasets with little variation in position, scale and saliency of the foreground object. As a consequence, we find that these methods struggle to cover the granularity and diversity possessed by real world images. To this end, we propose a Semantic Guidance pipeline with 1) a bi-level painting procedure for learning the distinction between foreground and background brush strokes at training time. 2) We also introduce invariance to the position and scale of the foreground object through a neural alignment model, which combines object localization and spatial transformer networks in an end to end manner, to zoom into a particular semantic instance. 3) The distinguishing features of the in-focus object are then amplified by maximizing a novel guided backpropagation based focus reward. The proposed agent does not require any supervision on human stroke-data and successfully handles variations in foreground object attributes, thus, producing much higher quality canvases for the CUB-200 Birds and Stanford Cars-196 datasets. Finally, we demonstrate the further efficacy of our method on complex datasets with multiple foreground object instances by evaluating an extension of our method on the challenging Virtual-KITTI dataset.</small></div>
				</td>
			</tr>


			<tr class="work_row">
				<td class="work_col_1">
					<a href="https://arxiv.org/abs/2011.12574" target="_blank"><img alt="sparse_est" class="project_logo" src="assets/publications/images/coinrun_cluster_feats.png"></a>
				</td>
				<td class="work_col_2">
					<h3> Enhanced Scene Specificity with Sparse Dynamic Value Estimation </h3>
					<!-- <ul>
						<li>Trained a Deep Reinforcement Learning Agent to navigate a banana world simulated in the Unity Environment.</li>
						<li>The underlying model was a Dueling Double Deep Q Network (DDQN) with prioritized experience replay.</li>
					</ul> -->
					<b>Jaskirat Singh</b> and <a href="https://scholar.google.com/citations?user=vNHqr3oAAAAJ&hl=en" target="_blank">Liang Zheng</a>, 2020 <br><br>
					[<a href="/assets/publications/sparse_est.pdf" target="_blank">Paper</a>] <br><br>

					<div align="justify"><small class="abstract"><b>Abstract:</b> Multi-scene reinforcement learning involves training the RL agent across multiple scenes / levels from the same task, and has become essential for many generalization applications. However, the inclusion of multiple scenes leads to an increase in sample variance for policy gradient computations, often resulting in suboptimal performance with the direct application of traditional methods (e.g. PPO, A3C). One strategy for variance reduction is to consider each scene as a distinct Markov decision process (MDP) and learn a joint value function dependent on both state s and MDP M. However, this is non-trivial as the agent is usually unaware of the underlying level at train / test times in multi-scene RL. Recently, Singh et al. [1] tried to address this by proposing a dynamic value estimation approach that models the true joint value function distribution as a Gaussian mixture model (GMM). In this paper, we argue that the error between the true scene-specific value function and the predicted dynamic estimate can be further reduced by progressively enforcing sparse cluster assignments once the agent has explored most of the state space. The resulting agents not only show significant improvements in the final reward score across a range of OpenAI ProcGen environments, but also exhibit increased navigation efficiency while completing a game level.</small></div>
				</td>
			</tr>

			<tr class="work_row">
				<td class="work_col_1">
					<a href="https://arxiv.org/abs/2005.12254" target="_blank"><img alt="dynamic_est" class="project_logo" src="assets/publications/images/dynamic_est2.png"></a>
				</td>
				<td class="work_col_2">
					<h3> Dynamic Value Estimation for Single-Task Multi-Scene Reinforcement Learning </h3>
					<!-- <ul>
						<li>Trained a Deep Reinforcement Learning Agent to navigate a banana world simulated in the Unity Environment.</li>
						<li>The underlying model was a Dueling Double Deep Q Network (DDQN) with prioritized experience replay.</li>
					</ul> -->
					<b>Jaskirat Singh</b> and <a href="https://scholar.google.com/citations?user=vNHqr3oAAAAJ&hl=en" target="_blank">Liang Zheng</a>, 2020 <br><br>
					[<a href="https://arxiv.org/abs/2005.12254" target="_blank">Paper</a>] <br><br>

					<div align="justify"><small class="abstract"><b>Abstract:</b> Training deep reinforcement learning agents on environments with multiple levels / scenes / conditions from the same task, has become essential for many applications aiming to achieve generalization and domain transfer from simulation to the real world. While such a strategy is helpful with generalization, the use of multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods, effectively, continue to view this collection of scenes as a single Markov decision process (MDP) and thus, learn a scene-generic value function V(s). However, we argue that the sample variance for a multi-scene environment is best minimized by treating each scene as a distinct MDP, and then learning a joint value function V(s,M) dependent on both state s and MDP M. We further demonstrate that the true joint value function (given a state), for a multi-scene environment, follows a multi-modal distribution which is not captured by traditional CNN / LSTM based critics. To this end, we propose a dynamic value estimation (DVE) technique, which approximates the true joint value function through an attention mechanism over multiple value function hypothesis / modes. The resulting agent is able to learn a more accurate and scene-specific value function (and hence the advantage function), leading to a lower sample variance. Our proposed approach is simple to accommodate with several existing implementations (like PPO, A3C) and results in consistent improvements for a range of OpenAI ProcGen environments and the AI2-THOR framework based visual navigation task.</small></div>
				</td>
			</tr>
		</tbody>
	</table>
</div>